{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CSC578B Presentation Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# September 21\n",
    "\n",
    "## ImageNet Classification with Deep Convolutional Neural Networks\n",
    "https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Initialize with a Gaussian (normal) distribution\n",
    "- Max pooling\n",
    "- Overlap pooling\n",
    "- Fully connected layers at end (w/ softmax)\n",
    "- => What exactly do activation functions do?\n",
    "- Small weight decay not only regularized, but also reduced error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing\n",
    "- Two types of error: top-1 and top-5\n",
    "    - Top-1 is when the most probable output is wrong\n",
    "    - Top-5 is when the correct label is not in the top-5 estimates\n",
    "- Can check the $k$ closest by Euclidean distance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Overlap Pooling\n",
    "\n",
    "- Pooling method where pool grids overlap\n",
    "- E.G., Using (3x3) pool grids and a stride of (2x2)\n",
    "- Decreased error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Local Response Normalization\n",
    "\n",
    "- What is this and how does it work?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Solving overfitting\n",
    "\n",
    "- Data augmentation\n",
    "- Dropout"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# September 23\n",
    "## Going Deeper with CNNs (GoogLeNet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The key insight is that they use 1x1 convolutional filters to flatten the channels. This makes using more filter types (1x1, 3x3, and 5x5 simultaneously) and more layers computationally feasible.\n",
    "\n",
    "- Each \"layer\" of the network has four parallel streams:\n",
    "  -1x1 conv\n",
    "  - 1x1 conv -> 3x3 conv\n",
    "  - 1x1 conv -> 5x5 conv\n",
    "  - 3x3 max pool -> 1x1 conv\n",
    "- The outputs of these filters are then concatenated to form the layer output\n",
    "\n",
    "They called these \"Inception\" layers after the movie."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# September 28\n",
    "## Deep Inside Convolutional Networks: Visualizing Image Classification Models and Saliency Maps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is the model looking at?\n",
    "\n",
    "- Use numerical optimization of images to visualize ConvNet models\n",
    "- Saliency map\n",
    "- Use gradient-based visualization method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class Model Visualization - MAP\n",
    "\n",
    "Idea: Trained classification CNN to produce an image\n",
    "\n",
    "Technique: Numerically generate image representative of the class in terms of score.\n",
    "\n",
    "\\begin{equation}\n",
    "  \\textrm{argmax}_l S_c(I) - \\lambda\n",
    "\\end{equation}\n",
    "\n",
    "$S_c(I)$ - score of class $c$, computed by classification layer of ConvNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saliency Maps - Spatial Support\n",
    "\n",
    "**Idea:** Create an image which ranks the pixels of $I_0$ based on influence on the score $S_c(I)$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Oct 5 - Group Normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Many people dislike BN / are confused by it\n",
    "- BN is dependent on batch size\n",
    "- BN is not good for online/microbatch training\n",
    "- There is a discrepancy between training and testing for BN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ghost Batch Normalization was a first attempt to address BN issues.\n",
    "\n",
    "Layer Normalization\n",
    "- Normalize the activations within each layer\n",
    "- Used on some NLP tasks\n",
    "\n",
    "Instance Normalization\n",
    "- Similar to layer norm, but just for one channel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Group Normalization\n",
    "\n",
    "Def - Normalize within each group of channels\n",
    "\n",
    "There is a `tfa.layers.GroupNormalization` layer in Tensorflow.\n",
    "\n",
    "In practice, GN performs worse than BN. => Add weight standardization when batch size is large."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# October 7\n",
    "# ADAM: Adaptive Moment Estimation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimization vs regularization\n",
    "\n",
    "- Regularization focuses on reducing the test or generalization error without affecting the initial training error\n",
    "- Optimizations focuses on reducing training time\n",
    "  - Gradient descent\n",
    "  - SGD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Derived from:\n",
    "- RSMProp - RMS propagation\n",
    "- AdaGrad - Uses the second moment with no decay to deal with parse features\n",
    "\n",
    "Typically outperforms other optimizers and works well in most cases whereas most other optimizers have situations where they don't perform well at all.\n",
    "\n",
    "Advantages:\n",
    "- Magnitudes of parameters updates are invariant to rescaling of gradient\n",
    "- Step sizes are approximately bounded by the step size hyperparamter; does not require a stationary objective\n",
    "- Works with sparse gradients\n",
    "- Naturally performs a form of step size annealing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "- Combines abilities of AdaGrad and RMSProp\n",
    "- Smal memory requirements\n",
    "- Tends to converge fast\n",
    "- Robust and useful in a wide range of problems"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Residual Learning for Image Recognition (ResNet)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Is improving network performance as simple as adding more layers?\n",
    "- Not really. Simply adding more layers does not generally improve performance.\n",
    "\n",
    "Solution: instead of just stacking layers, re-add the identity every few layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# October 12\n",
    "\n",
    "#\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}