{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Distillation Presentation Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Distilling the Knowledge in a Neural Network\n",
    "\n",
    "> Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. 2015 Mar 9."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The main idea\n",
    "\n",
    "We can train large, cumbersome models and then using another kind of training, which the authoers call \"distillation\", we can transfer the knowledge from the large model to a smaller one more suitable for deployment. This is especially effective to distill the knowledge in an ensemble of models to a single small model that is more suitable for inference."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Process (?)\n",
    "\n",
    "If I am understanding the paper correctly, the distillation process is to use the individual class probabilities obtained from a modified softmax function with a raised temperature as \"soft targets\" (non-one hot targets) for training a smaller model.\n",
    "\n",
    "An additional benefit of this is that it allows the use of unlabeled data in the transfer process since the data labels will be obtained when running it through the large network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Distillation\n",
    "\n",
    "From the paper:\n",
    "Neural networks typically produce class probabilities by using a \"softmax\" output layer that converts the logit, $z_i$, computed for each class into a probability, $q_i$, by comparing $z_i$ with the other logits.\n",
    "\n",
    "\\begin{equation}\n",
    "  q_i = \\frac{\\exp (z_i / T)}{\\sum_j \\exp(z_j/T)}\n",
    "\\end{equation}\n",
    "where $T$ is a temperature that is normally set to 1. Using a higher value for $t$ produces a softer probability distribution over the classes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the paper:\n",
    "In the simplest form of distillation, knowledge is transferred to the distilled model by training it on a transfer set and using a soft target distribution for each case in the transfer set that is produced by using the cumbersome model with a high temperature in its softmax. **The same high temperature is used when training the distilled model, but after it has been trained it uses a temperature of 1.**\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}