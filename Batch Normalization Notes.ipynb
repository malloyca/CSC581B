{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
    "By Sergey Ioffe and Christian Szegedy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Ioffe, S., & Szegedy, C. (2015, June).\n",
    "> Batch normalization: Accelerating deep network training by reducing internal covariate shift.\n",
    "> In International conference on machine learning (pp. 448-456). PMLR.\n",
    "\n",
    "These are my summarization notes from the paper.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Stochastic gradient descent (SGD) is an effective way to train deep neural nets.\n",
    "SGD optimizes the parameters $\\Theta$ of the network in order to minimize the loss:\n",
    "\\begin{equation}\n",
    "    \\Theta = \\textrm{argmin}_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N \\ell \\left( x_i, \\Theta \\right),\n",
    "\\end{equation}\n",
    "where $x_1, \\dots, x_N$ is the training set.\n",
    "In SGD, the training takes place in steps, with each step considering a *mini-batch* $x_1,\\dots,x_m$ of size $m$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using mini-batches instead of one training sample at a time has several benefits.\n",
    "First, the gradient of the loss for a mini-batch is an estimate of the gradient over the whole training set.\n",
    "The quality of the estimation improves as the batch size increases.\n",
    "Second, computation over a mini-batch can be more efficient than $m$ computations for individual samples.\n",
    "\n",
    "SGD works well, but requires careful tuning of the model hyper-parameters.\n",
    "The inputs to each layer are affected by the parameters of all preceding layers.\n",
    "Small changes to the network parameters amplify as the network becomes deeper.\n",
    "\n",
    "Layers must continuously adapt to the change in distributions of the inputs.\n",
    "This is an issue.\n",
    "When the input distribution to a learning system changes, it is said to experience *covariate shift*.\n",
    "Parts of a network (e.g., a sub-network or a layer) can also experience covariate shift.\n",
    "\n",
    "Since normalization helps the network generalize, applying it to a sub-network will also help.\n",
    "This means that the distribution of $x$ will remain stable over time and then the parameters of the sub-network\n",
    "do not have to compensate for changes in the distribution of $x$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This also helps layers outside of the sub-network.\n",
    "If a layer has a sigmoid activation function $g(x) = \\frac{1}{1 + \\exp(-x)}$, then as $|x|$ increases,\n",
    "$g'(x)$ tends towards zero.\n",
    "Thus for all dimensions except those with small absolute values,\n",
    "the gradient will vanish and the model will train slowly.\n",
    "Over time, changes to the weights and biases will cause many dimensions of $x$ to saturate.\n",
    "This effect is amplified as the network depth increases.\n",
    "However, ensuring that nonlinearity inputs remain more stable during training,\n",
    "then the optimizer is less likely to get stuck in the saturated regime and this would accelerate training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Internal Covariate Shift\n",
    "\n",
    "Internal covariate shift is the change in the distribution of nodes of a deep network in the course of training.\n",
    "Training speed increases by reducing/eliminating internal covariate shift.\n",
    "**Batch normalization** is a mechanism that reduces internal covariate shift.\n",
    "\n",
    "This works by applying normalization to fix the means and variances of layer inputs.\n",
    "It also reduces the dependence of gradients on the scale or initial value of parameters.\n",
    "This allows for higher learning rates without risk of divergence.\n",
    "This also regularizes the model and reduces the need for Dropout.\n",
    "This also allows for the use of saturating nonlinearities since it prevents getting stuck in the saturated modes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Towards Reducing Internal Covariate Shift"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}