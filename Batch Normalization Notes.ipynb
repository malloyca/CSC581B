{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
    "By Sergey Ioffe and Christian Szegedy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Ioffe, S., & Szegedy, C. (2015, June).\n",
    "> Batch normalization: Accelerating deep network training by reducing internal covariate shift.\n",
    "> In International conference on machine learning (pp. 448-456). PMLR.\n",
    "\n",
    "These are my summarization notes from the paper.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Stochastic gradient descent (SGD) is an effective way to train deep neural nets.\n",
    "SGD optimizes the parameters $\\Theta$ of the network in order to minimize the loss:\n",
    "\\begin{equation}\n",
    "    \\Theta = \\textrm{argmin}_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N \\ell \\left( x_i, \\Theta \\right),\n",
    "\\end{equation}\n",
    "where $x_1, \\dots, x_N$ is the training set.\n",
    "In SGD, the training takes place in steps, with each step considering a *mini-batch* $x_1,\\dots,x_m$ of size $m$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using mini-batches instead of one training sample at a time has several benefits.\n",
    "First, the gradient of the loss for a mini-batch is an estimate of the gradient over the whole training set.\n",
    "The quality of the estimation improves as the batch size increases.\n",
    "Second, computation over a mini-batch can be more efficient than $m$ computations for individual samples.\n",
    "\n",
    "SGD works well, but requires careful tuning of the model hyper-parameters.\n",
    "The inputs to each layer are affected by the parameters of all preceding layers.\n",
    "Small changes to the network parameters amplify as the network becomes deeper.\n",
    "\n",
    "Layers must continuously adapt to the change in distributions of the inputs.\n",
    "This is an issue.\n",
    "When the input distribution to a learning system changes, it is said to experience *covariate shift*.\n",
    "Parts of a network (e.g., a sub-network or a layer) can also experience covariate shift.\n",
    "\n",
    "Since normalization helps the network generalize, applying it to a sub-network will also help.\n",
    "This means that the distribution of $x$ will remain stable over time and then the parameters of the sub-network\n",
    "do not have to compensate for changes in the distribution of $x$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This also helps layers outside of the sub-network.\n",
    "If a layer has a sigmoid activation function $g(x) = \\frac{1}{1 + \\exp(-x)}$, then as $|x|$ increases,\n",
    "$g'(x)$ tends towards zero.\n",
    "Thus for all dimensions except those with small absolute values,\n",
    "the gradient will vanish and the model will train slowly.\n",
    "Over time, changes to the weights and biases will cause many dimensions of $x$ to saturate.\n",
    "This effect is amplified as the network depth increases.\n",
    "However, ensuring that nonlinearity inputs remain more stable during training,\n",
    "then the optimizer is less likely to get stuck in the saturated regime and this would accelerate training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Internal Covariate Shift\n",
    "\n",
    "Internal covariate shift is the change in the distribution of nodes of a deep network in the course of training.\n",
    "Training speed increases by reducing/eliminating internal covariate shift.\n",
    "**Batch normalization** is a mechanism that reduces internal covariate shift.\n",
    "\n",
    "This works by applying normalization to fix the means and variances of layer inputs.\n",
    "It also reduces the dependence of gradients on the scale or initial value of parameters.\n",
    "This allows for higher learning rates without risk of divergence.\n",
    "This also regularizes the model and reduces the need for Dropout.\n",
    "This also allows for the use of saturating nonlinearities since it prevents getting stuck in the saturated modes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Towards Reducing Internal Covariate Shift"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Internal Covariate Shift* is the change in the distribution of network activations due to the change in network parameters during training. The intuition for this comes from the knowledge that networks converge faster if the inputs are standardized - that is adjusted to have zero mean and unit variance.\n",
    "\n",
    "Batch normalization wants to ensure that, for any parameter values, the network *always* produces activations with the desired distribution since this would all the gradient of the loss with respect to the model parameters toa ccount for the normalization, and for its dependence on the model parameters $\\Theta$. If $x$ is a layer input and $X$ is the set of inputs over the training data set, then the normalization can be written as a transformation\n",
    "\\begin{equation}\n",
    "    \\hat{x} = \\textrm{Norm}(x,X).\n",
    "\\end{equation}\n",
    "This depends not only on the given training example $x$, but on all examples $X$ - each of which depends on $\\Theta$ (the set of parameters) if $x$ is generated by another layer. For backpropagation, you would compute\n",
    "$\\frac{\\partial \\textrm{Norm}(x,X)}{\\partial x}$ and\n",
    "$\\frac{\\partial \\textrm{Norm}(x,X)}{\\partial X}$. However, this is computationally expensive so the authors looked for another method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Normalization via Mini-Batch Statistics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is computationally costly to fully standardize each layer's inputs (with respect to the full training set), so batch normalization makes two simplifications. First, it standardizes each feature independently (each feature is normalized to have zero mean and unit variance.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{x}^{(k)} = \\frac{x^{(k)} - E(x^{(k)})} {\\sqrt{ \\textrm{Var}(x^{(k)}) }}\n",
    "\\end{equation}\n",
    "\n",
    "is computed over the training data set.\n",
    "\n",
    "The issue with this, however, is that normalizing the inputs of a layer may change what the layer can represent. For example, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. To address, the authors introduce, for each activation $x^{(k)}$, a pair of parameters $\\gamma^{(k)}$ and $\\beta^{(k)}$ that scale and shift the normalized value:\n",
    "\n",
    "\\begin{equation}\n",
    "    y^{(k)} = \\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}.\n",
    "\\end{equation}\n",
    "\n",
    "The $\\gamma$ and $\\beta$ parameters are learned along with the original model. In fact, if $\\gamma = \\sqrt{\\textrm{Var}(x)}$ and $\\beta = E(x)$ then you would recover the original activations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second simplification is to use the training batch (used for stochastic gradient descent) for normalization. Since we are already using each mini-batch to approximate the training set, we can use each mini-batch to estimate the mean and variance of each activation. This also allows normalization to fully participate in backpropagation.\n",
    "\n",
    "The use of mini-batches is enabled by computation of per-dimension variances rather than joint covariances. If joint covariances were being used, regularization would be required since the mini-batch size is likely to be smaller than the number of activations, resulting in singular covariance matrices.\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "Input: Values of $x$ over a mini-batch: $B = \\left\\{ x_1, \\dots, x_m \\right\\}$; parameters to be learned: $\\gamma,\\beta$.\n",
    "\n",
    "Output: $\\{ y_i = \\textrm{BN}_{\\gamma,\\beta} (x_i) \\}$\n",
    "\n",
    "Mini-batch mean: $\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i=1}^m x_i$\n",
    "\n",
    "Mini-batch variance: $ \\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 $\n",
    "\n",
    "Normalize: $ \\hat{x}_i \\leftarrow \\frac{ x_i - \\mu_B }{ \\sqrt{\\sigma_B^2 + \\epsilon} }$\n",
    "\n",
    "Scale and shift: $ y_i \\leftarrow \\gamma \\hat{x}_i + \\beta \\equiv \\textrm{BN}_{\\gamma,\\beta}(x_i) $"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "During training, it will be necessary to backpropagate the gradient of the loss, $\\ell$, through the batch normalization transform and compute the gradients with respect to the parameters of the BN transform. Using the chain rule, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\ell}{\\partial \\hat{x}_i} = \\frac{\\partial \\ell}{\\partial y_i} \\cdot \\gamma\\\\\n",
    "    \\frac{\\partial\\ell}{\\partial \\sigma_B^2} = \\sum_{i=1}^m \\frac{\\partial\\ell}{\\partial \\hat{x}_i} \\cdot\n",
    "    \\left( x_i - \\mu_B \\right) \\cdot \\frac{-1}{2} \\left(\\sigma_B^2 + \\epsilon\\right)^{-3/2}\\\\\n",
    "    \\frac{\\partial\\ell}{\\partial\\mu_B} = \\sum_{i=1}^m \\frac{\\partial\\ell}{\\partial\\hat{x}_i} \\cdot\n",
    "    \\frac{-1}{\\sqrt{ \\sigma_B^2 + \\epsilon }}\\\\\n",
    "    \\frac{\\partial\\ell}{\\partial x_i} = \\frac{\\partial\\ell}{\\partial \\hat{x}_i} \\cdot\n",
    "    \\frac{1}{\\sqrt{ \\sigma_B^2 + \\epsilon }} + \\frac{\\partial\\ell}{\\partial\\sigma_B^2} \\cdot\n",
    "    \\frac{2 (x_i - \\mu_B)}{m} + \\frac{\\partial\\ell}{\\partial\\mu_B} \\cdot \\frac{1}{m}\\\\\n",
    "    \\frac{\\partial\\ell}{\\partial\\gamma} = \\sum_{i=1}^m \\frac{\\partial\\ell}{\\partial y_i} \\cdot \\hat{x}_i\\\\\n",
    "    \\frac{\\partial\\ell}{\\partial\\beta} = \\sum_{i=1}^m \\frac{\\partial\\ell}{\\partial y_i}\n",
    "\\end{equation}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Train and Inference with Batch-Normalized Networks\n",
    "\n",
    "To **Batch-Normalize** a network, a BN layer is inserted. A layer that previously received $x$ as the input, now receives $BN(x) = \\gamma\\hat{x} + \\beta$.\n",
    "\n",
    "Batch Normalization can be used with batch gradient descent, SGD with size $m>1$, or variants such as Adagrad.\n",
    "\n",
    "Normalizing based on the mini-batch is efficient for training, but is not desirable for inference. Once the network has been trained, we use the normalization\n",
    "\\begin{equation}\n",
    "    \\hat{x} = \\frac{x - E(x)}{\\sqrt{x} + \\epsilon},\n",
    "\\end{equation}\n",
    "using the population instead of the mini-batches, for inference."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Batch-Normalized Convolutional Networks\n",
    "\n",
    "This works for both convolutional and fully connected layers. Add the BN transform immediately before the nonlinearity by normalizing $x = Wu + b$.\n",
    "\n",
    "For convolutional layers, we additionally want the normalization to obey the convolutional property - so that different elements of the same feature map, at different locations, are normalized in the same way. To achieve this, we jointly normalize all the activations in a mini-batch, over all locations.\n",
    "\n",
    "During inference, the BN transform applies the same linear transformation to each activation in a given feature map."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Batch Normalization enables higher learning rates\n",
    "\n",
    "In non-BN networks, too high a learning rate may result in exploding or vanishing gradients or getting stuck in poor local minima. BN helps address this since BN prevents small changes in layer parameters from amplifying as the data propagates.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Experiments\n",
    "### 4.1 Activations over time\n",
    "\n",
    "To verify they tested on MNIST with a simple network:\n",
    "- Input layer (28x28 binary images)\n",
    "- 3 fully connected layers with 100 activations (neurons)\n",
    "  - Each layer computes $y = g(Wu + b)$ with sigmoid nonlinearity and $W$ initalized to small random Gaussian values\n",
    "- Fully connected layer with 10 neurons (one per class)\n",
    "- Uses cross-entropy loss\n",
    "- Added BN to each hidden layer of the network for testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 ImageNet classification\n",
    "\n",
    "Applied BN to a new variant of Inception network (GoogLeNet) trained on the ImageNet classification task.\n",
    "- Large number of convolutional and pooling layers\n",
    "- Softmax layer to predict the image class out of 1000 possibilities\n",
    "- Conv layers use ReLU\n",
    "- Main difference to the original version is that the $5\\times5$ layers were replaced by two consecutive layers of $3\\times3$ convolutions with up to 128 filters.\n",
    "- Contains $13.6\\times 10^6$ (13 million) parameters\n",
    "- Only the softmax layer is fully connected\n",
    "- BN applied to the input of each nonlinearity in a convolutional way\n",
    "\n",
    "#### 4.2.1 Accelerating BN Networks\n",
    "In order to fully tack advantage of BN they used the following modifications:\n",
    "- Increased learning rate\n",
    "- Remove dropout\n",
    "  - Authors conjecture that BN provides similar regularization benefits as Dropout\n",
    "- Shuffle training examples more thoroughly\n",
    "  - This is to make sure that different mini-batches have as little overlap as possible\n",
    "- Reduce $L_2$ weight regularization\n",
    "- Accelerate the learning rate decay\n",
    "  - Since network trains faster, it was necessary to increase learning rate\n",
    "- Remove Local Response Normalization\n",
    "- Reduce the photometric distortions\n",
    "  - I believe they are referring to images distorted for data augmentation purposes\n",
    "  - Since training happens faster, they want to let the trainer focus on more \"real\" images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2.2 Single-Network Classification\n",
    "\n",
    "They tested it by training various architectures on the LSVRC2012 training data and tested on the validation data. The architectures tested were:\n",
    "\n",
    "- **Inception**\n",
    "  - the modified version described before\n",
    "  - Initial learning rate of 0.0015\n",
    "- **BN-Baseline**\n",
    "  - Inception + BN before each nonlinearity\n",
    "- **BN-x5**\n",
    "  - Inception + BN + modifications from 4.2.1\n",
    "  - Initial learning rate is 5x greater at 0.0075\n",
    "  - Using this new learning rate with original Inception cause model parameters to reach machine infinity\n",
    "- **BN-x30**\n",
    "  - Like BN-x5, but with initial learning rate of 0.045 (30x that of Inception)\n",
    "- **BN-x5-Sigmoid**\n",
    "  - Like BN-x5, but with sigmoid nonlinearity instead of ReLU\n",
    "  - Using sigmoid with original Inception cuases it's accuracy to be equivalent to chance.\n",
    "  -\n",
    "\n",
    "**Include figs 2, 3 from paper**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2.3 Ensemble Classification\n",
    "\n",
    "Also tested using ensemble models. The previous best top-5 error was 4.94% reported by He et al. Using an ensemble of BN networks, the authors were able to beat the then best score on ILSVRC2012 with a score of 4.82%.\n",
    "\n",
    "The modifications were as follows:\n",
    "- 6 networks each based on BN-x30\n",
    "- Increased initial weights in conv layers\n",
    "- Used Dropout (probability of 5% or 10% vs 40% for original Inception)\n",
    "- Used non-conv BN with last hidden layers of each model\n",
    "\n",
    "Each network took about $6\\times 10^6$ training steps to achieve max accuracy.\n",
    "\n",
    "**Include fig 4 from paper**\n",
    "\n",
    "I don't know what the differences are between all of the architectures. Let's just allow this to be another piece of validation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "- BN is a novel mechanism for dramatically accelerating training.\n",
    "- Covariate shift is known to complicate training\n",
    "  - This also applies to sub-networks and layers\n",
    "- Normalizing the input aids in training\n",
    "  - $\\Rightarrow$ Normalizing at internal activations should aid in training as well\n",
    "- Networks are typically trained with some form of stochastic optimization\n",
    "  - $\\Rightarrow$ perform normalization for each mini-batch and backpropagate gradients through normalization parameters\n",
    "- BN adds only two extra parameters per activation\n",
    "- Allows the use of saturating nonlinearities\n",
    "- Makes networks more tolerant to increased training rates\n",
    "- Often negates need for Dropout for regularization\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Outline for presentation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Intro\n",
    "- Normalization\n",
    "  - How does it work?\n",
    "  - What is the problem with just input normalization?\n",
    "- Batch Normalization\n",
    "  - The intuition leading to it\n",
    "  - How does it work?\n",
    "  - How is it implemented?\n",
    "- Example showing performance improvements\n",
    "  - Basic nueral net (not a CNN)\n",
    "  - ImageNet\n",
    "  - Ensemble Classification\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}