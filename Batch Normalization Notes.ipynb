{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
    "By Sergey Ioffe and Christian Szegedy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Ioffe, S., & Szegedy, C. (2015, June).\n",
    "> Batch normalization: Accelerating deep network training by reducing internal covariate shift.\n",
    "> In International conference on machine learning (pp. 448-456). PMLR.\n",
    "\n",
    "These are my summarization notes from the paper.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Stochastic gradient descent (SGD) is an effective way to train deep neural nets.\n",
    "SGD optimizes the parameters $\\Theta$ of the network in order to minimize the loss:\n",
    "\\begin{equation}\n",
    "    \\Theta = \\textrm{argmin}_{\\Theta} \\frac{1}{N} \\sum_{i=1}^N \\ell \\left( x_i, \\Theta \\right),\n",
    "\\end{equation}\n",
    "where $x_1, \\dots, x_N$ is the training set.\n",
    "In SGD, the training takes place in steps, with each step considering a *mini-batch* $x_1,\\dots,x_m$ of size $m$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using mini-batches instead of one training sample at a time has several benefits.\n",
    "First, the gradient of the loss for a mini-batch is an estimate of the gradient over the whole training set.\n",
    "The quality of the estimation improves as the batch size increases.\n",
    "Second, computation over a mini-batch can be more efficient than $m$ computations for individual samples.\n",
    "\n",
    "SGD works well, but requires careful tuning of the model hyper-parameters.\n",
    "The inputs to each layer are affected by the parameters of all preceding layers.\n",
    "Small changes to the network parameters amplify as the network becomes deeper.\n",
    "\n",
    "Layers must continuously adapt to the change in distributions of the inputs.\n",
    "This is an issue.\n",
    "When the input distribution to a learning system changes, it is said to experience *covariate shift*.\n",
    "Parts of a network (e.g., a sub-network or a layer) can also experience covariate shift.\n",
    "\n",
    "Since normalization helps the network generalize, applying it to a sub-network will also help.\n",
    "This means that the distribution of $x$ will remain stable over time and then the parameters of the sub-network\n",
    "do not have to compensate for changes in the distribution of $x$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This also helps layers outside of the sub-network.\n",
    "If a layer has a sigmoid activation function $g(x) = \\frac{1}{1 + \\exp(-x)}$, then as $|x|$ increases,\n",
    "$g'(x)$ tends towards zero.\n",
    "Thus for all dimensions except those with small absolute values,\n",
    "the gradient will vanish and the model will train slowly.\n",
    "Over time, changes to the weights and biases will cause many dimensions of $x$ to saturate.\n",
    "This effect is amplified as the network depth increases.\n",
    "However, ensuring that nonlinearity inputs remain more stable during training,\n",
    "then the optimizer is less likely to get stuck in the saturated regime and this would accelerate training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Internal Covariate Shift\n",
    "\n",
    "Internal covariate shift is the change in the distribution of nodes of a deep network in the course of training.\n",
    "Training speed increases by reducing/eliminating internal covariate shift.\n",
    "**Batch normalization** is a mechanism that reduces internal covariate shift.\n",
    "\n",
    "This works by applying normalization to fix the means and variances of layer inputs.\n",
    "It also reduces the dependence of gradients on the scale or initial value of parameters.\n",
    "This allows for higher learning rates without risk of divergence.\n",
    "This also regularizes the model and reduces the need for Dropout.\n",
    "This also allows for the use of saturating nonlinearities since it prevents getting stuck in the saturated modes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Towards Reducing Internal Covariate Shift"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Internal Covariate Shift* is the change in the distribution of network activations due to the change in network parameters during training. The intuition for this comes from the knowledge that networks converge faster if the inputs are standardized - that is adjusted to have zero mean and unit variance.\n",
    "\n",
    "Batch normalization wants to ensure that, for any parameter values, the network *always* produces activations with the desired distribution since this would all the gradient of the loss with respect to the model parameters toa ccount for the normalization, and for its dependence on the model parameters $\\Theta$. If $x$ is a layer input and $X$ is the set of inputs over the training data set, then the normalization can be written as a transformation\n",
    "\\begin{equation}\n",
    "    \\hat{x} = \\textrm{Norm}(x,X).\n",
    "\\end{equation}\n",
    "This depends not only on the given training example $x$, but on all examples $X$ - each of which depends on $\\Theta$ (the set of parameters) if $x$ is generated by another layer. For backpropagation, you would compute\n",
    "$\\frac{\\partial \\textrm{Norm}(x,X)}{\\partial x}$ and\n",
    "$\\frac{\\partial \\textrm{Norm}(x,X)}{\\partial X}$. However, this is computationally expensive so the authors looked for another method."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Normalization via Mini-Batch Statistics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is computationally costly to fully standardize each layer's inputs, so batch normalization makes two simplifications. First, it standardizes each feature independently (each feature is normalized to have zero mean and unit variance.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{x}^{(k)} = \\frac{x^{(k)} - E(x^{(k)})} {\\sqrt{ \\textrm{Var}(x^{(k)}) }}\n",
    "\\end{equation}\n",
    "\n",
    "is computed over the training data set.\n",
    "\n",
    "The issue with this, however, is that normalizing the inputs of a layer may change what the layer can represent. For example, normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity. To address, the authors introduce, for each activation $x^{(k)}$, a pair of parameters $\\gamma^{(k)}$ and $\\beta^{(k)}$ that scale and shift the normalized value:\n",
    "\n",
    "\\begin{equation}\n",
    "    y^{(k)} = \\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}.\n",
    "\\end{equation}\n",
    "\n",
    "The $\\gamma$ and $\\beta$ parameters are learned along with the original model. In fact, if $\\gamma = \\sqrt{\\textrm{Var}(x)}$ and $\\beta = E(x)$ then you would recover the original activations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second simplification is to use the training batch (used for stochastic gradient descent) for normalization. Since we are already using each mini-batch to approximate the training set, we can use each mini-batch to estimate the mean and variance of each activation. This also allows normalization to fully participate in backpropagation.\n",
    "\n",
    "The use of mini-batches is enabled by computation of per-dimension variances rather than joint covariances. If joint covariances were being used, regularization would be required since the mini-batch size is likely to be smaller than the number of activations, resulting in singular covariance matrices.\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "Input: Values of $x$ over a mini-batch: $B = \\left\\{ x_1, \\dots, x_m \\right\\}$; parameters to be learned: $\\gamma,\\beta$.\n",
    "\n",
    "Output: $\\{ y_i = \\textrm{BN}_{\\gamma,\\beta} (x_i) \\}$\n",
    "\n",
    "Mini-batch mean: $\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i=1}^m x_i$\n",
    "\n",
    "Mini-batch variance: $ \\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 $\n",
    "\n",
    "Normalize: $ \\hat{x}_i \\leftarrow \\frac{ x_i - \\mu_B }{ \\sqrt{\\sigma_B^2 + \\epsilon} }$\n",
    "\n",
    "Scale and shift: $ y_i \\leftarrow \\gamma \\hat{x}_i + \\beta \\equiv \\textrm{BN}_{\\gamma,\\beta}(x_i) $"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "During training, it will be necessary to backpropagate the gradient of the loss, $\\ell$, through the batch normalization transform and compute the gradients with respect to the parameters of the BN transform. Using the chain rule, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\ell}{\\partial \\hat{x}_i} = \\frac{\\partial \\ell}{\\partial y_i} \\cdot \\gamma\\\\\n",
    "    \\frac{\\partial\\ell}{\\partial \\sigma_B^2} = \\sum_{i=1}^m \\frac{\\partial\\ell}{\\partial \\hat{x}_i} \\cdot\n",
    "    \\left( x_i - \\mu_B \\right) \\cdot \\frac{-1}{2} \\left(\\sigma_B^2 + \\epsilon\\right)^{-3/2}\\\\\n",
    "    \\frac{\\partial\\ell}{\\partial\\mu_B} = \\sum_{i=1}^m \\frac{\\partial\\ell}{\\partial\\hat{x}_i} \\cdot\n",
    "    \\frac{-1}{\\sqrt{ \\sigma_B^2 + \\epsilon }}\\\\\n",
    "    \\frac{\\partial\\ell}{\\partial x_i} = \\frac{\\partial\\ell}{\\partial \\hat{x}_i} \\cdot\n",
    "    \\frac{1}{\\sqrt{ \\sigma_B^2 + \\epsilon }} + \\frac{\\partial\\ell}{\\partial\\sigma_B^2} \\cdot\n",
    "    \\frac{2 (x_i - \\mu_B)}{m} + \\frac{\\partial\\ell}{\\partial\\mu_B} \\cdot \\frac{1}{m}\n",
    "\\end{equation}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}