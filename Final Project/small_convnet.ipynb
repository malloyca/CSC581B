{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "small_convnet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOpkK3inUbSr0bjyAvw/VHS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malloyca/CSC581B/blob/main/Final%20Project/small_convnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PEXOKNd_15u"
      },
      "source": [
        "# Notes\n",
        "\n",
        "- Complete trial with eight layer model\n",
        "- Add batch normalization\n",
        "- Add more feedback to early stopping\n",
        "  - accuracy and loss for that epoch\n",
        "- Ideas for improving the performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI3XLmYiD-UJ"
      },
      "source": [
        "# imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Compose, Normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuXW8boJEcrA",
        "outputId": "366bd389-4121-4032-a9cd-d021b7717ea8"
      },
      "source": [
        "# Load the training data (CIFAR10 to start)\n",
        "training_data = datasets.CIFAR100(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "# Load the test data\n",
        "test_data = datasets.CIFAR100(\n",
        "    root = \"data\",\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor()\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyVJ8ne8xjKt"
      },
      "source": [
        "training_targets = training_data.targets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMQ119G7xmDK"
      },
      "source": [
        "train_split_index, valid_split_index = train_test_split(\n",
        "    np.arange(len(training_targets)), test_size=0.2, stratify=training_targets\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ2dy9pmEwz6"
      },
      "source": [
        "batch_size = 100\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size,\n",
        "                              sampler=SubsetRandomSampler(train_split_index))\n",
        "valid_dataloader = DataLoader(training_data, batch_size=batch_size,\n",
        "                              sampler=SubsetRandomSampler(valid_split_index))\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE6plIy9E2D_",
        "outputId": "9d623608-4e48-49f9-8a2a-fffb80adf2d4"
      },
      "source": [
        "# Check that it is splitting the data properly\n",
        "train_length = 0\n",
        "for _, y in train_dataloader:\n",
        "  train_length += len(y)\n",
        "print(f\"Length of training split: {train_length}\")\n",
        "\n",
        "valid_length = 0\n",
        "for _, y in valid_dataloader:\n",
        "  valid_length += len(y)\n",
        "print(f\"Length of validation split: {valid_length}\")\n",
        "\n",
        "test_length = 0\n",
        "for _, y in test_dataloader:\n",
        "  test_length += len(y)\n",
        "print(f\"Length of test split: {test_length}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training split: 40000\n",
            "Length of validation split: 10000\n",
            "Length of test split: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG4cwo9Ox9-Z",
        "outputId": "5c215a9d-7fd1-4821-96ab-a45874078098"
      },
      "source": [
        "# Check that there are 100 instances of each class in the validation set\n",
        "count = 0\n",
        "for _, y in valid_dataloader:\n",
        "  for target in y:\n",
        "    if int(target.numpy()) == 0:\n",
        "      count += 1\n",
        "\n",
        "print(count)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGQGFOcoFAeO"
      },
      "source": [
        "# Building basic convolutional neural nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuGLhBqQE_xI",
        "outputId": "2417aa7d-b87e-4234-9c2b-9d6162ad9832"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H5TNXTJE4Nd"
      },
      "source": [
        "# Define the model\n",
        "class basicConvNet(nn.Module):\n",
        "  def __init__(self, name):\n",
        "    super(basicConvNet, self).__init__()\n",
        "    self.name = name\n",
        "    self.basic_conv_net = nn.Sequential(\n",
        "        # 1st layer\n",
        "        nn.Conv2d(3, 50, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(50),\n",
        "        nn.ReLU(),\n",
        "        #nn.Dropout2d(p=0.2),\n",
        "        \n",
        "        # 2nd layer\n",
        "        nn.Conv2d(50, 75, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(75),\n",
        "        nn.ReLU(),\n",
        "        #nn.Dropout2d(p=0.2),\n",
        "        nn.MaxPool2d(2),\n",
        "        \n",
        "        # 3rd layer - 16x16\n",
        "        nn.Conv2d(75, 100, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(100),\n",
        "        nn.ReLU(),\n",
        "        #nn.Dropout2d(p=0.2),\n",
        "        \n",
        "        # 4th layer\n",
        "        nn.Conv2d(100, 150, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(150),\n",
        "        nn.ReLU(),\n",
        "        #nn.Dropout2d(p=0.2),\n",
        "        nn.MaxPool2d(2),\n",
        "        \n",
        "        # 5th layer - 8x8\n",
        "        nn.Conv2d(150, 200, (3,3), padding='same'),\n",
        "        #nn.BatchNorm2d(200),\n",
        "        nn.ReLU(),\n",
        "        #nn.Dropout2d(p=0.2),\n",
        "        \n",
        "        # 6th layer\n",
        "        nn.Conv2d(200, 250, (3,3), padding='same'),\n",
        "        #nn.BatchNorm2d(250),\n",
        "        nn.ReLU(),\n",
        "        #nn.Dropout2d(p=0.2),\n",
        "        nn.MaxPool2d(2),\n",
        "        \n",
        "        # 7th layer - 4x4\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(250*4*4, 1000),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(p=0.2),\n",
        "        \n",
        "        # 8th layer\n",
        "        nn.Linear(1000, 500),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.2),\n",
        "        nn.Linear(500, 100)\n",
        "    )\n",
        "\n",
        "  # Forward propagation\n",
        "  def forward(self, x):\n",
        "    x = self.basic_conv_net(x)\n",
        "    #x = self.flatten(x)\n",
        "    #x = self.output(x)\n",
        "    return x\n",
        "      "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQGk95EKWan5"
      },
      "source": [
        "# Training loop\n",
        "def train(dataloader, batch_size, model, loss_fn, optimizer):\n",
        "  num_batches = len(dataloader)\n",
        "  size = num_batches * batch_size\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backprop\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I_S627HXLWz"
      },
      "source": [
        "# Test function\n",
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "  return test_loss"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zYjxXdFgV9z"
      },
      "source": [
        "# Validation function\n",
        "def validation(dataloader, batch_size, model, loss_fn):\n",
        "  num_batches = len(dataloader)\n",
        "  size = num_batches * batch_size\n",
        "  model.eval()\n",
        "  val_loss, num_correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      val_loss += loss_fn(pred, y).item()\n",
        "      num_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  val_loss /= num_batches\n",
        "  accuracy = num_correct / size\n",
        "  print(f\"Validation Error: \\n Validation accuracy: {(100 * accuracy):>0.1f}%, Validation loss: {val_loss:>8f} \\n\")\n",
        "  return val_loss"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfMapknHYDYO"
      },
      "source": [
        "n_epochs = 20"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uypo30igYE2G"
      },
      "source": [
        "def training_loop(n_epochs, model, train_data, valid_data, batch_size,\n",
        "                  loss_function, optimizer, early_stopping=False, patience=10):\n",
        "  current_epoch = 0\n",
        "  best_epoch = 0\n",
        "  best_loss = float('inf')\n",
        "  patience_counter = 0\n",
        "\n",
        "  for e in range(n_epochs):\n",
        "    print(f\"\\nEpoch {e+1}\\n----------------------------\")\n",
        "    train(train_data, batch_size, model, loss_function, optimizer)\n",
        "    val_loss = validation(valid_data, batch_size, model, loss_function)\n",
        "\n",
        "    # Iterate epoch counter\n",
        "    current_epoch += 1\n",
        "\n",
        "    # If early_stopping check test_loss\n",
        "    if early_stopping:\n",
        "      # case: test loss beats the current best loss\n",
        "      if val_loss < best_loss:\n",
        "        # store loss\n",
        "        best_loss = val_loss\n",
        "\n",
        "        # reset patience counter\n",
        "        patience_counter = 0\n",
        "\n",
        "        # store model and epoch number\n",
        "        print(\"Storing new best model.\")\n",
        "        best_model_state_dict = copy.deepcopy(model.state_dict)\n",
        "        best_epoch = current_epoch\n",
        "\n",
        "      # Case: patience limit not yet reached => iterate patience counter\n",
        "      elif patience_counter < patience - 1:\n",
        "        patience_counter += 1\n",
        "        print(f\"Patience count: {patience_counter}\")\n",
        "\n",
        "      # Case: patience limit reached\n",
        "      else:\n",
        "        print(\"Finished due to early stopping.\")\n",
        "        print(f\"Saving best model: {model.name}_epoch-{best_epoch:03d}\")\n",
        "        torch.save(best_model_state_dict, f'{model.name}_epoch-{best_epoch:03d}')\n",
        "        break\n",
        "\n",
        "  # If we get here, we did not stop early - save best model\n",
        "  if early_stopping:\n",
        "    print(f\"Saving best model: {model.name}_epoch-{best_epoch:03d}\")\n",
        "    torch.save(best_model_state_dict, f'{model.name}_epoch-{best_epoch:03d}')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLQYaHdZTnJK",
        "outputId": "dfa690b0-5c50-4b76-ca56-5efb22cf3dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = basicConvNet('eight-layer').to(device)\n",
        "model"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "basicConvNet(\n",
              "  (basic_conv_net): Sequential(\n",
              "    (0): Conv2d(3, 50, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(50, 75, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (4): BatchNorm2d(75, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(75, 100, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (8): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU()\n",
              "    (10): Conv2d(100, 150, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (11): BatchNorm2d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU()\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(150, 200, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (15): ReLU()\n",
              "    (16): Conv2d(200, 250, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
              "    (17): ReLU()\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Flatten(start_dim=1, end_dim=-1)\n",
              "    (20): Linear(in_features=4000, out_features=1000, bias=True)\n",
              "    (21): ReLU()\n",
              "    (22): Dropout2d(p=0.2, inplace=False)\n",
              "    (23): Linear(in_features=1000, out_features=500, bias=True)\n",
              "    (24): ReLU()\n",
              "    (25): Dropout(p=0.2, inplace=False)\n",
              "    (26): Linear(in_features=500, out_features=100, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deD1sei_WLmX"
      },
      "source": [
        "# Loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxAb2cyfCYXq",
        "outputId": "890292f2-ea6d-40de-e66f-5c62e38ddfef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_loop(n_epochs, model, train_dataloader, valid_dataloader, batch_size,\n",
        "              loss_fn, optimizer, early_stopping=True, patience=5)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "----------------------------\n",
            "loss: 4.604773 [    0/40000]\n",
            "loss: 4.180986 [10000/40000]\n",
            "loss: 3.922232 [20000/40000]\n",
            "loss: 3.951463 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 13.9%, Validation loss: 3.596256 /n\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 2\n",
            "----------------------------\n",
            "loss: 3.708282 [    0/40000]\n",
            "loss: 3.477298 [10000/40000]\n",
            "loss: 3.309785 [20000/40000]\n",
            "loss: 3.141798 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 22.9%, Validation loss: 3.136150 /n\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 3\n",
            "----------------------------\n",
            "loss: 3.037631 [    0/40000]\n",
            "loss: 2.932130 [10000/40000]\n",
            "loss: 2.811244 [20000/40000]\n",
            "loss: 2.936870 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 28.4%, Validation loss: 2.819392 /n\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 4\n",
            "----------------------------\n",
            "loss: 2.564445 [    0/40000]\n",
            "loss: 2.623851 [10000/40000]\n",
            "loss: 2.828455 [20000/40000]\n",
            "loss: 2.871089 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 31.6%, Validation loss: 2.687131 /n\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 5\n",
            "----------------------------\n",
            "loss: 2.227190 [    0/40000]\n",
            "loss: 2.551515 [10000/40000]\n",
            "loss: 2.418025 [20000/40000]\n",
            "loss: 2.389152 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 34.8%, Validation loss: 2.555241 /n\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 6\n",
            "----------------------------\n",
            "loss: 2.201770 [    0/40000]\n",
            "loss: 2.179860 [10000/40000]\n",
            "loss: 2.159611 [20000/40000]\n",
            "loss: 2.310217 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 37.2%, Validation loss: 2.476292 /n\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 7\n",
            "----------------------------\n",
            "loss: 1.824648 [    0/40000]\n",
            "loss: 1.933166 [10000/40000]\n",
            "loss: 1.986427 [20000/40000]\n",
            "loss: 1.676796 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 37.9%, Validation loss: 2.431747 /n\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 8\n",
            "----------------------------\n",
            "loss: 1.822360 [    0/40000]\n",
            "loss: 1.333490 [10000/40000]\n",
            "loss: 1.725188 [20000/40000]\n",
            "loss: 2.089839 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 37.8%, Validation loss: 2.548096 /n\n",
            "Patience count: 1\n",
            "\n",
            "Epoch 9\n",
            "----------------------------\n",
            "loss: 1.819330 [    0/40000]\n",
            "loss: 1.593037 [10000/40000]\n",
            "loss: 1.623009 [20000/40000]\n",
            "loss: 1.445346 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 39.4%, Validation loss: 2.500429 /n\n",
            "Patience count: 2\n",
            "\n",
            "Epoch 10\n",
            "----------------------------\n",
            "loss: 1.325065 [    0/40000]\n",
            "loss: 1.479542 [10000/40000]\n",
            "loss: 1.322949 [20000/40000]\n",
            "loss: 1.619645 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 39.3%, Validation loss: 2.569408 /n\n",
            "Patience count: 3\n",
            "\n",
            "Epoch 11\n",
            "----------------------------\n",
            "loss: 1.078172 [    0/40000]\n",
            "loss: 1.135178 [10000/40000]\n",
            "loss: 1.191019 [20000/40000]\n",
            "loss: 1.496739 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 38.6%, Validation loss: 2.667354 /n\n",
            "Patience count: 4\n",
            "\n",
            "Epoch 12\n",
            "----------------------------\n",
            "loss: 0.820767 [    0/40000]\n",
            "loss: 1.098362 [10000/40000]\n",
            "loss: 1.093144 [20000/40000]\n",
            "loss: 0.989182 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 39.1%, Validation loss: 2.838222 /n\n",
            "Finished due to early stopping.\n",
            "Saving best model: eight-layer_epoch-007\n",
            "Saving best model: eight-layer_epoch-007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag8mS2tK0Sl-"
      },
      "source": [
        "# Results on CIFAR100:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iflio0cX0U_5"
      },
      "source": [
        "## Eight layer model\n",
        "\n",
        "### architecture\n",
        "        # 1st layer\n",
        "        nn.Conv2d(3, 50, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(50),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(p=0.2),\n",
        "        # 2nd layer\n",
        "        nn.Conv2d(50, 75, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(75),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(p=0.2),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 3rd layer - 16x16\n",
        "        nn.Conv2d(75, 100, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(100),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(p=0.2),\n",
        "        # 4th layer\n",
        "        nn.Conv2d(100, 200, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(200),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(p=0.2),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 5th layer - 8x8\n",
        "        nn.Conv2d(200, 100, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(100),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(p=0.2),\n",
        "        # 6th layer\n",
        "        nn.Conv2d(100, 75, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(75),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(p=0.2),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 7th layer - 4x4\n",
        "        nn.Conv2d(75, 50, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(50),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(p=0.2),\n",
        "        # 8th layer\n",
        "        nn.Conv2d(50, 50, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(50),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(p=0.2),\n",
        "    )\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.output = nn.Linear(50*4*4, 100)\n",
        "\n",
        "### Trial\n",
        "- SGD lr = 5e-2\n",
        "- Accuracy: 10.1%, Avg loss: 1.910425  @ epoch 56\n",
        "\n",
        "---\n",
        "### Architecture\n",
        "        # 1st layer\n",
        "        nn.Conv2d(3, 50, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(50),\n",
        "        nn.ReLU(),\n",
        "        # 2nd layer\n",
        "        nn.Conv2d(50, 75, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(75),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 3rd layer - 16x16\n",
        "        nn.Conv2d(75, 100, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(100),\n",
        "        nn.ReLU(),\n",
        "        # 4th layer\n",
        "        nn.Conv2d(100, 200, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(200),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 5th layer - 8x8\n",
        "        nn.Conv2d(200, 100, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(100),\n",
        "        nn.ReLU(),\n",
        "        # 6th layer\n",
        "        nn.Conv2d(100, 75, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(75),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 7th layer - 4x4\n",
        "        nn.Conv2d(75, 50, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(50),\n",
        "        nn.ReLU(),\n",
        "        # 8th layer\n",
        "        nn.Conv2d(50, 50, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(50),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.output = nn.Linear(50*4*4, 100)\n",
        "\n",
        "### Trial\n",
        "- SGD lr = 1e-1\n",
        "- Accuracy: 9.3%, Avg loss: 2.115571 @ epoch 7\n",
        "\n",
        "\n",
        "### Architecture\n",
        "        # 1st layer\n",
        "        nn.Conv2d(3, 256, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        # 2nd layer\n",
        "        nn.Conv2d(256, 128, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 3rd layer - 16x16\n",
        "        nn.Conv2d(128, 64, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        # 4th layer\n",
        "        nn.Conv2d(64, 32, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 5th layer - 8x8\n",
        "        nn.Conv2d(32, 32, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        # 6th layer\n",
        "        nn.Conv2d(32, 16, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 7th layer - 4x4\n",
        "        nn.Conv2d(16, 16, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        # 8th layer\n",
        "        nn.Conv2d(16, 16, (3,3), padding='same'),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "\n",
        "### Results\n",
        "- LR = 1e-1\n",
        "- Accuracy: 8.2%, Avg loss: 2.274223 @ 11 epochs => decrease learning rate\n",
        "\n",
        "\n",
        "### Architecture\n",
        "        # 1st layer\n",
        "        nn.Conv2d(3, 256, (3,3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        # 2nd layer\n",
        "        nn.Conv2d(256, 128, (3,3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 3rd layer - 16x16\n",
        "        nn.Conv2d(128, 64, (3,3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        # 4th layer\n",
        "        nn.Conv2d(64, 32, (3,3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 5th layer - 8x8\n",
        "        nn.Conv2d(32, 32, (3,3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        # 6th layer\n",
        "        nn.Conv2d(32, 16, (3,3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        # 7th layer - 4x4\n",
        "        nn.Conv2d(16, 16, (3,3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        # 8th layer\n",
        "        nn.Conv2d(16, 16, (3,3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "\n",
        "### Trials\n",
        "- Accuracy: 6.6%, Avg loss: 2.713449 @ 27 epochs\n",
        "\n",
        "## Six layer model\n",
        "\n",
        "### Architecture\n",
        "- 32 x 32 x 256 conv, ReLU - (3,3), 256\n",
        "- Maxpool (2,2)\n",
        "- 16 x 16 x 128 conv, ReLU - (3,3), 128\n",
        "- Maxpool (2,2)\n",
        "- 8 x 8 x 64 conv, ReLU - (3,3), 64\n",
        "- Maxpool (2,2)\n",
        "- 4 x 4 x 32 conv, ReLU - (3,3), 32\n",
        "- 4 x 4 x 16 conv, ReLU - (3,3), 16\n",
        "- Flatten\n",
        "- Fully connect layer (256, 100)\n",
        "\n",
        "### Trials\n",
        "- 7.3% / 2.499734\n",
        "  - SGD\n",
        "  - `lr = 5e-2`\n",
        "  - Validation accuracy - 7.3% @ 20 epochs\n",
        "  - Avg loss - 2.499734\n",
        "- 7.4% / 2.490613\n",
        "  - SGD\n",
        "  - `lr = 1e-2`\n",
        "  - Validation accuracy - 7.4% @ 65 epochs\n",
        "  - Avg loss - 2.490613\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK31_tvQLkRo"
      },
      "source": [
        "# Results on CIFAR10:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53YZcWbELoEm"
      },
      "source": [
        "## Six layer model (current best)\n",
        "- Test accuracy 72.7%\n",
        "- Avg loss 1.733851\n",
        "\n",
        "### Architecture\n",
        "    # 1st layer\n",
        "    nn.Conv2d(3, 256, (3,3), padding='same'),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    # 2nd layer\n",
        "    nn.Conv2d(256, 128, (3,3), padding='same'),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    # 3rd layer\n",
        "    nn.Conv2d(128, 64, (3,3), padding='same'),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    # 4th layer\n",
        "    nn.Conv2d(64, 32, (3,3), padding='same'),\n",
        "    nn.ReLU(),\n",
        "    #nn.MaxPool2d((2,2)),\n",
        "    # 5th layer\n",
        "    nn.Conv2d(32, 32, (3,3), padding='same'),\n",
        "    nn.ReLU(),\n",
        "    #nn.MaxPool2d((2,2)),\n",
        "    # 6th layer\n",
        "    nn.Conv2d(32, 16, (3,3), padding='same'),\n",
        "    nn.ReLU(),"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imBDYEwbYp2V"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}