{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOXoWlNnk7+8xB0sBSqhFSz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d3ad58ccd974b5ea3c3a994d45d88b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e94146ffd5574fbd83b7731d74745c9f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_49a6c98e48dc4d53b50a587e798ec53f",
              "IPY_MODEL_4bf53ea705ea47d685d9db0da1819983",
              "IPY_MODEL_d3454359130b4f2b94211f7243acd843"
            ]
          }
        },
        "e94146ffd5574fbd83b7731d74745c9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49a6c98e48dc4d53b50a587e798ec53f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_039ba06af3e24f08a7fc1b9ae9695f14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db6d649beb364e048d65c262b8ae9436"
          }
        },
        "4bf53ea705ea47d685d9db0da1819983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ef915f5277b7405c9950e0dc509363c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 169001437,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 169001437,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6e0f387de22422bb6190d82fd8baad3"
          }
        },
        "d3454359130b4f2b94211f7243acd843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b04cf0e6d5554a4489a901de34a3c8ea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169001984/? [00:03&lt;00:00, 67166591.73it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f6fc5d7903b46e0b7cd9769a8089776"
          }
        },
        "039ba06af3e24f08a7fc1b9ae9695f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db6d649beb364e048d65c262b8ae9436": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef915f5277b7405c9950e0dc509363c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6e0f387de22422bb6190d82fd8baad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b04cf0e6d5554a4489a901de34a3c8ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f6fc5d7903b46e0b7cd9769a8089776": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malloyca/CSC581B/blob/main/Final%20Project/resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXGGquxpklHx"
      },
      "source": [
        "# CSC581B - Deep Learning for Image Classification\n",
        "# Final Project: ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxQbek_mkYIL"
      },
      "source": [
        "# imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import numpy as np\n",
        "import random\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnUNUXWuNIDQ"
      },
      "source": [
        "Set up data augmentation transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC4xQ73UpdXv"
      },
      "source": [
        "# data augmentation transforms\n",
        "data_augmentation_transform = transforms.Compose([\n",
        "  transforms.RandomHorizontalFlip(p=0.5),\n",
        "  transforms.RandomRotation(degrees=30),\n",
        "  transforms.RandomCrop(32, padding=2),\n",
        "  transforms.ColorJitter(brightness=0.25, contrast=0.5, saturation=0.25, hue = 0.15),\n",
        "  transforms.RandomGrayscale(p=0.2),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(\n",
        "      mean = [0.5071, 0.4867, 0.4408],\n",
        "      std = [0.2675, 0.2565, 0.2761]\n",
        "  ),\n",
        "  transforms.RandomErasing(p=.35),\n",
        "])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3RpBinzNM9E"
      },
      "source": [
        "# non-augmentation transform (with data normalization)\n",
        "normalize_transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(\n",
        "      mean = [0.5071, 0.4867, 0.4408],\n",
        "      std = [0.2675, 0.2565, 0.2761]\n",
        "  )\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "6d3ad58ccd974b5ea3c3a994d45d88b5",
            "e94146ffd5574fbd83b7731d74745c9f",
            "49a6c98e48dc4d53b50a587e798ec53f",
            "4bf53ea705ea47d685d9db0da1819983",
            "d3454359130b4f2b94211f7243acd843",
            "039ba06af3e24f08a7fc1b9ae9695f14",
            "db6d649beb364e048d65c262b8ae9436",
            "ef915f5277b7405c9950e0dc509363c9",
            "a6e0f387de22422bb6190d82fd8baad3",
            "b04cf0e6d5554a4489a901de34a3c8ea",
            "6f6fc5d7903b46e0b7cd9769a8089776"
          ]
        },
        "id": "hKDxCL01NQEj",
        "outputId": "6c73dd3e-b601-4a74-9f78-18cbb0727ab0"
      },
      "source": [
        "# Load the training data (CIFAR10 to start)\n",
        "training_data_with_augmentation = CIFAR100(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = data_augmentation_transform,\n",
        ")\n",
        "\n",
        "training_data_without_augmentation = CIFAR100(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = normalize_transform,\n",
        ")\n",
        "\n",
        "# this is necessary to prevent the data augmentation transforms from being applied to the validation set\n",
        "validation_data = CIFAR100(\n",
        "    root = \"data\",\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = normalize_transform,\n",
        ")\n",
        "\n",
        "# Load the test data\n",
        "test_data = CIFAR100(\n",
        "    root = \"data\",\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transforms.ToTensor()\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d3ad58ccd974b5ea3c3a994d45d88b5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyopx_A3rhyv"
      },
      "source": [
        "training_targets = training_data_with_augmentation.targets"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgqJ5Ko6rsAg"
      },
      "source": [
        "train_split_index, valid_split_index = train_test_split(\n",
        "    np.arange(len(training_targets)), test_size=0.2, stratify=training_targets\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYz4SRz2rj6b"
      },
      "source": [
        "batch_size = 100"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_xiX43FrlUx"
      },
      "source": [
        "# Create data loaders\n",
        "train_augmentation_dataloader = DataLoader(training_data_with_augmentation, batch_size=batch_size,\n",
        "                              sampler=SubsetRandomSampler(train_split_index))\n",
        "train_no_augmentation_dataloader = DataLoader(training_data_without_augmentation, batch_size=batch_size,\n",
        "                              sampler=SubsetRandomSampler(train_split_index))\n",
        "valid_dataloader = DataLoader(validation_data, batch_size=batch_size,\n",
        "                              sampler=SubsetRandomSampler(valid_split_index))\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOLh79JTNX6I"
      },
      "source": [
        "train_status = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDREfgQTrnG4"
      },
      "source": [
        "if not train_status:\n",
        "  # Check that it is splitting the data properly\n",
        "  train_length = 0\n",
        "  for _, y in train_augmentation_dataloader:\n",
        "    train_length += len(y)\n",
        "  print(f\"Length of training (augmentation) split: {train_length}\")\n",
        "\n",
        "  train_aug_length = 0\n",
        "  for _, y in train_no_augmentation_dataloader:\n",
        "    train_aug_length += len(y)\n",
        "  print(f\"Length of training (no augmentation) split: {train_aug_length}\")\n",
        "\n",
        "  valid_length = 0\n",
        "  for _, y in valid_dataloader:\n",
        "    valid_length += len(y)\n",
        "  print(f\"Length of validation split: {valid_length}\")\n",
        "\n",
        "  test_length = 0\n",
        "  for _, y in test_dataloader:\n",
        "    test_length += len(y)\n",
        "  print(f\"Length of test split: {test_length}\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zOWIfAiryju"
      },
      "source": [
        "if not train_status:\n",
        "  # Check that there are 100 instances of a random class in the validation set\n",
        "  count = 0\n",
        "  test_class = random.randint(0,99)\n",
        "  for _, y in valid_dataloader:\n",
        "    for target in y:\n",
        "      if int(target.numpy()) == test_class:\n",
        "        count += 1\n",
        "\n",
        "  print(count)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8dWkMFC-r72A",
        "outputId": "6580ddf4-890c-4ff7-9130-7d33c23bdef7"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cpu'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdtbf10er4Pp"
      },
      "source": [
        "# Build ResNet for CIFAR100\n",
        "\n",
        "This is based on section 4.2 of the ResNet paper.\n",
        "\n",
        "The first layer is $3 \\times 3$ convolutions. Then they use a stack of $6n$ layers with $3\\times3$ convolutions on the feature maps of sizes $\\{32,16,8\\}$ respectively with $2n$ layers for each feature map size.\n",
        "\n",
        "The first residual block of each stack (except the first) downsamples by setting the stride to 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peBSa5KKr2zz"
      },
      "source": [
        "# Define the ResNet Block\n",
        "class ResidualBlock(nn.Module):\n",
        "  '''\n",
        "  Class for building the ResNet blocks. The architecture here follows the\n",
        "  specifications from section 4.2 of the ResNet paper.\n",
        "  '''\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_channels,\n",
        "      output_channels,\n",
        "      stride\n",
        "  ):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.block = nn.Sequential(\n",
        "        nn.Conv2d(input_channels, output_channels, kernel_size=3, stride=stride, padding=1),\n",
        "        nn.BatchNorm2d(output_channels),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(output_channels, output_channels, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(output_channels)\n",
        "    )\n",
        "    self.relu = nn.ReLU()\n",
        "    self.identity = nn.Sequential()\n",
        "    if stride == 2:\n",
        "      self.identity = nn.Sequential(\n",
        "          Identity(output_channels - input_channels)\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "  # forward propagation\n",
        "  def forward(self, x):\n",
        "    # residual block: y = F(x, w) + x\n",
        "    out = self.block(x)\n",
        "    out += self.identity(x)\n",
        "    out = self.relu(out)\n",
        "    return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3xOHNEkpVUs"
      },
      "source": [
        "class Identity(nn.Module):\n",
        "  '''\n",
        "  This is a class to allow me to use nn.functional.pad inside of a squential block.\n",
        "  '''\n",
        "  def __init__(self,padding):\n",
        "    super(Identity, self).__init__()\n",
        "    self.identity = nn.functional.pad\n",
        "    self.padding = padding\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(\"Dim going into identity:\", list(x.size()))\n",
        "    x = self.identity(x[:,:,::2,::2], pad=(0,0,0,0,0,self.padding))\n",
        "    #print(\"Dim coming out of identity:\", list(x.size()))\n",
        "    return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1aixmK9FUgl"
      },
      "source": [
        "# This is based on section 4.2 of the ResNet paper\n",
        "# number of filters is {16, 32, 64} for the three stacks\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      block,\n",
        "      num_blocks,\n",
        "      name = 'ResNet',\n",
        "  ):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.num_filters = [16, 32, 64]\n",
        "    self.name = name\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.stack1 = self.build_residual_block(block, self.num_filters[0], num_blocks[0], first_stride=1)\n",
        "    self.stack2 = self.build_residual_block(block, self.num_filters[1], num_blocks[1], first_stride=2) # set first stride to 2 to downsample\n",
        "    self.stack3 = self.build_residual_block(block, self.num_filters[2], num_blocks[2], first_stride=2)\n",
        "\n",
        "    self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear = nn.Linear(64, 100)\n",
        "\n",
        "\n",
        "  def build_residual_block(self, block, num_filters, num_blocks, first_stride):\n",
        "    strides = [first_stride]\n",
        "    layers = []\n",
        "    for _ in range(num_blocks-1):\n",
        "      strides.append(1)\n",
        "    for stride in strides:\n",
        "      if stride == 2:\n",
        "        layers.append(block(\n",
        "            input_channels=num_filters//2,\n",
        "            output_channels=num_filters,\n",
        "            stride=stride,\n",
        "        ))\n",
        "      else:\n",
        "        layers.append(block(\n",
        "            input_channels=num_filters,\n",
        "            output_channels=num_filters,\n",
        "            stride=stride,\n",
        "        ))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    #print(\"Dim going into stack1:\", list(out.size()))\n",
        "    out = self.stack1(out)\n",
        "    #print(\"Dim going into stack2:\", list(out.size()))\n",
        "    out = self.stack2(out)\n",
        "    #print(\"Dim going into stack3:\", list(out.size()))\n",
        "    out = self.stack3(out)\n",
        "    #print(\"Dim going into linear:\", list(out.size()))\n",
        "    out = self.avg_pool(out)\n",
        "    out = self.flatten(out)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NucWACfFrwE8"
      },
      "source": [
        "def resnet_test():\n",
        "  return ResNet(ResidualBlock, [5, 5, 5], name='test')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-vVUFsqOjNz"
      },
      "source": [
        "def resnet20():\n",
        "  return ResNet(ResidualBlock, [3, 3, 3], name='resnet20')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lUoJWFiOvAp"
      },
      "source": [
        "def resnet32():\n",
        "  return ResNet(ResidualBlock, [5, 5, 5], name='resnet32')"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRI8rnGSO3QP"
      },
      "source": [
        "def resnet44():\n",
        "  return ResNet(ResidualBlock, [7, 7, 7], name='resnet44')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMZbDNFqmGmH",
        "outputId": "7f4576be-a315-431f-f7e4-b79576d726c8"
      },
      "source": [
        "model = resnet_test().to(device)\n",
        "model"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU()\n",
              "  (stack1): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "    (3): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "    (4): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (stack2): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential(\n",
              "        (0): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "    (3): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "    (4): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (stack3): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential(\n",
              "        (0): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "    (3): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "    (4): ResidualBlock(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (relu): ReLU()\n",
              "      (identity): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (linear): Linear(in_features=64, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUnudSdofwG_"
      },
      "source": [
        "# Training setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKzlSWQvfvht"
      },
      "source": [
        "# Training function\n",
        "def train(dataloader, batch_size, model, loss_fn, optimizer):\n",
        "  num_batches = len(dataloader)\n",
        "  size = num_batches * batch_size\n",
        "  model.train()\n",
        "  train_loss, num_correct = 0, 0\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backprop\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Track train_loss and accuracy\n",
        "    train_loss += loss.item()\n",
        "    num_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "  train_loss /= num_batches\n",
        "  accuracy = num_correct / size\n",
        "  return train_loss, accuracy"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FCvYiajfE5v"
      },
      "source": [
        "# Validation function\n",
        "def validation(dataloader, batch_size, model, loss_fn):\n",
        "  num_batches = len(dataloader)\n",
        "  size = num_batches * batch_size\n",
        "  model.eval()\n",
        "  val_loss, num_correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      val_loss += loss_fn(pred, y).item()\n",
        "      num_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  val_loss /= num_batches\n",
        "  accuracy = num_correct / size\n",
        "  print(f\"Validation Error: \\n Validation accuracy: {(100 * accuracy):>0.1f}%, Validation loss: {val_loss:>8f} \\n\")\n",
        "  return val_loss, accuracy"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2O0Sxarf1Qe"
      },
      "source": [
        "# Test function\n",
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "  return test_loss"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OVpJnZJf5M5"
      },
      "source": [
        "def training_loop(n_epochs, model, train_data, valid_data, batch_size,\n",
        "                  loss_function, optimizer, scheduler=None,\n",
        "                  early_stopping=False, patience=10):\n",
        "  current_epoch = 0\n",
        "  best_epoch = 0\n",
        "  best_loss = float('inf')\n",
        "  patience_counter = 0\n",
        "\n",
        "  train_losses = []\n",
        "  train_accuracies = []\n",
        "  val_losses = []\n",
        "  val_accuracies = []\n",
        "\n",
        "  for e in range(n_epochs):\n",
        "    print(f\"\\nEpoch {e+1}\\n----------------------------\")\n",
        "    # Iterate epoch counter\n",
        "    current_epoch += 1\n",
        "\n",
        "    train_loss, train_accuracy = train(train_data, batch_size, model, loss_function, optimizer)\n",
        "    val_loss, val_accuracy = validation(valid_data, batch_size, model, loss_function)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Iterate scheduler (this is set up for ReduceLROnPlateau)\n",
        "    if scheduler is not None:\n",
        "      scheduler.step(val_loss)\n",
        "\n",
        "    # If early_stopping check test_loss\n",
        "    if early_stopping:\n",
        "      # case: test loss beats the current best loss\n",
        "      if val_loss < best_loss:\n",
        "        # store loss\n",
        "        best_loss = val_loss\n",
        "\n",
        "        # reset patience counter\n",
        "        patience_counter = 0\n",
        "\n",
        "        # store model and epoch number\n",
        "        print(\"Storing new best model.\")\n",
        "        best_model_state_dict = copy.deepcopy(model.state_dict)\n",
        "        best_epoch = current_epoch\n",
        "\n",
        "      # Case: patience limit not yet reached => iterate patience counter\n",
        "      elif patience_counter < patience - 1:\n",
        "        patience_counter += 1\n",
        "        print(f\"Patience count: {patience_counter}\")\n",
        "\n",
        "      # Case: patience limit reached\n",
        "      else:\n",
        "        print(\"Finished due to early stopping.\")\n",
        "        print(f\"Saving best model: {model.name}_epoch-{best_epoch:03d}\")\n",
        "        torch.save(best_model_state_dict, f'{model.name}_epoch-{best_epoch:03d}')\n",
        "        break\n",
        "\n",
        "  # If we get here, we did not stop early - save best model\n",
        "  if early_stopping:\n",
        "    print(f\"Saving best model: {model.name}_epoch-{best_epoch:03d}\")\n",
        "    torch.save(best_model_state_dict, f'{model.name}_epoch-{best_epoch:03d}')\n",
        "  else:\n",
        "    print()\n",
        "\n",
        "  return train_losses, train_accuracies, val_losses, val_accuracies"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDFDIhpJgUOi"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a34SpCqKf3q0"
      },
      "source": [
        "n_epochs = 10"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uXWvG36s0HW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6301d95-330d-4552-c9b6-9ed5e51d6cb5"
      },
      "source": [
        "model_20 = resnet20().to(device)\n",
        "num_param = sum(p.numel() for p in model_20.parameters())\n",
        "print(f\"Total number of parameters for ResNet20: {num_param:,}\")\n",
        "\n",
        "model_32 = resnet32().to(device)\n",
        "num_param = sum(p.numel() for p in model_32.parameters())\n",
        "print(f\"Total number of parameters for ResNet32: {num_param:,}\")\n",
        "\n",
        "model_44 = resnet44().to(device)\n",
        "num_param = sum(p.numel() for p in model_44.parameters())\n",
        "print(f\"Total number of parameters for ResNet44: {num_param:,}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters for ResNet20: 276,260\n",
            "Total number of parameters for ResNet32: 471,140\n",
            "Total number of parameters for ResNet44: 666,020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2yza2axPiPL",
        "outputId": "b1015a1a-01df-4dbb-920e-27e15711a3c7"
      },
      "source": [
        "model_20_aug = resnet20().to(device)\n",
        "num_param = sum(p.numel() for p in model_20_aug.parameters())\n",
        "print(f\"Total number of parameters for ResNet20: {num_param:,}\")\n",
        "\n",
        "model_32_aug = resnet32().to(device)\n",
        "num_param = sum(p.numel() for p in model_32_aug.parameters())\n",
        "print(f\"Total number of parameters for ResNet32: {num_param:,}\")\n",
        "\n",
        "model_44_aug = resnet44().to(device)\n",
        "num_param = sum(p.numel() for p in model_44_aug.parameters())\n",
        "print(f\"Total number of parameters for ResNet44: {num_param:,}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters for ResNet20: 276,260\n",
            "Total number of parameters for ResNet32: 471,140\n",
            "Total number of parameters for ResNet44: 666,020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Usx1-MwLf7Tb"
      },
      "source": [
        "# Loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer_20 = torch.optim.Adam(model_20.parameters(), lr=1e-3)\n",
        "optimizer_20_aug = torch.optim.Adam(model_20_aug.parameters(), lr=1e-3)\n",
        "optimizer_32 = torch.optim.Adam(model_32.parameters(), lr=1e-3)\n",
        "optimizer_32_aug = torch.optim.Adam(model_32_aug.parameters(), lr=1e-3)\n",
        "optimizer_44 = torch.optim.Adam(model_44.parameters(), lr=1e-3)\n",
        "optimizer_44_aug = torch.optim.Adam(model_44_aug.parameters(), lr=1e-3)\n",
        "\n",
        "# LR scheduling\n",
        "scheduler_20 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_20, mode='min', patience=5)\n",
        "scheduler_20_aug = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_20_aug, mode='min', patience=5)\n",
        "scheduler_32 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_32, mode='min', patience=5)\n",
        "scheduler_32_aug = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_32_aug, mode='min', patience=5)\n",
        "scheduler_44 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_44, mode='min', patience=5)\n",
        "scheduler_44_aug = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer_44_aug, mode='min', patience=5)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8sBYfvYgMMP",
        "outputId": "13cb0cc7-9851-4952-ac98-1f0552d349d6"
      },
      "source": [
        "train_loss_20, train_accuracy_20, val_loss_20, val_accuracy_20 = training_loop(n_epochs, model_20, train_no_augmentation_dataloader, valid_dataloader, batch_size,\n",
        "              loss_fn, optimizer_20, scheduler_20, early_stopping=True, patience=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "----------------------------\n",
            "loss: 5.236021 [    0/40000]\n",
            "loss: 4.100166 [10000/40000]\n",
            "loss: 3.737955 [20000/40000]\n",
            "loss: 3.605719 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 15.6%, Validation loss: 3.426668 \n",
            "\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 2\n",
            "----------------------------\n",
            "loss: 3.572837 [    0/40000]\n",
            "loss: 3.482478 [10000/40000]\n",
            "loss: 3.285691 [20000/40000]\n",
            "loss: 3.018946 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 23.4%, Validation loss: 2.997034 \n",
            "\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 3\n",
            "----------------------------\n",
            "loss: 2.603859 [    0/40000]\n",
            "loss: 2.810353 [10000/40000]\n",
            "loss: 2.557725 [20000/40000]\n",
            "loss: 2.645427 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 31.2%, Validation loss: 2.647825 \n",
            "\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 4\n",
            "----------------------------\n",
            "loss: 2.373348 [    0/40000]\n",
            "loss: 2.384575 [10000/40000]\n",
            "loss: 2.245880 [20000/40000]\n",
            "loss: 2.302922 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 35.7%, Validation loss: 2.413171 \n",
            "\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 5\n",
            "----------------------------\n",
            "loss: 2.021951 [    0/40000]\n",
            "loss: 2.259663 [10000/40000]\n",
            "loss: 1.895599 [20000/40000]\n",
            "loss: 2.416871 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 39.5%, Validation loss: 2.308434 \n",
            "\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 6\n",
            "----------------------------\n",
            "loss: 1.801984 [    0/40000]\n",
            "loss: 1.676830 [10000/40000]\n",
            "loss: 2.001786 [20000/40000]\n",
            "loss: 2.085735 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 40.4%, Validation loss: 2.256514 \n",
            "\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 7\n",
            "----------------------------\n",
            "loss: 1.918444 [    0/40000]\n",
            "loss: 1.984053 [10000/40000]\n",
            "loss: 2.011883 [20000/40000]\n",
            "loss: 1.839869 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 41.0%, Validation loss: 2.232219 \n",
            "\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 8\n",
            "----------------------------\n",
            "loss: 1.443608 [    0/40000]\n",
            "loss: 1.909273 [10000/40000]\n",
            "loss: 1.702238 [20000/40000]\n",
            "loss: 1.708519 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 42.1%, Validation loss: 2.179996 \n",
            "\n",
            "Storing new best model.\n",
            "\n",
            "Epoch 9\n",
            "----------------------------\n",
            "loss: 1.481350 [    0/40000]\n",
            "loss: 1.657235 [10000/40000]\n",
            "loss: 1.188103 [20000/40000]\n",
            "loss: 1.695243 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 41.8%, Validation loss: 2.255010 \n",
            "\n",
            "Patience count: 1\n",
            "\n",
            "Epoch 10\n",
            "----------------------------\n",
            "loss: 1.507033 [    0/40000]\n",
            "loss: 1.338318 [10000/40000]\n",
            "loss: 1.603991 [20000/40000]\n",
            "loss: 1.601452 [30000/40000]\n",
            "Validation Error: \n",
            " Validation accuracy: 44.8%, Validation loss: 2.106157 \n",
            "\n",
            "Storing new best model.\n",
            "Saving best model: ResNet_epoch-010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uULVr_LSgYfx"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(f\"Train and Validation Loss for {model_20.name}\")\n",
        "plt.plot(val_loss_20, label=\"val_loss\")\n",
        "plt.plot(train_loss_20, label=\"train_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N49Ao8VZQTAk"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(f\"Train and Validation Accuracy for {model_20.name}\")\n",
        "plt.plot(val_accuracy_20, label=\"val_accuracy\")\n",
        "plt.plot(train_accuracy_20, label=\"train_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2PXCuIwTNvt"
      },
      "source": [
        "train_loss_20_aug, train_accuracy_20_aug, val_loss_20_aug, val_accuracy_20_aug = training_loop(n_epochs, model_20_aug, train_augmentation_dataloader, valid_dataloader, batch_size,\n",
        "              loss_fn, optimizer_20_aug, scheduler_20_aug, early_stopping=True, patience=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hpwM4JdThwB"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(f\"Train and Validation Loss for {model_20_aug.name}\")\n",
        "plt.plot(val_loss_20_aug, label=\"val_loss\")\n",
        "plt.plot(train_loss_20_aug, label=\"train_loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDfR2AlsTjdd"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(f\"Train and Validation Accuracy for {model_20_aug.name}\")\n",
        "plt.plot(val_accuracy_20_aug, label=\"val_accuracy\")\n",
        "plt.plot(train_accuracy_20_aug, label=\"train_accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}